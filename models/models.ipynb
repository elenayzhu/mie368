{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3ec55a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LassoCV, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, r2_score, mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    HistGradientBoostingClassifier,\n",
    "    HistGradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    RandomForestClassifier,\n",
    "    StackingClassifier,\n",
    ")\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "265e9122",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data_with_clusters.csv\"\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    \"MONTH\",\n",
    "    \"HOUR\",\n",
    "    \"origin_flights_day\",\n",
    "    \"airline_bucket\",\n",
    "    \"origin_bucket\",\n",
    "    \"destination_bucket\",\n",
    "    \"lagged_delay_flag\",\n",
    "    \"prev_real_delay\",\n",
    "]\n",
    "\n",
    "TARGET_CLF = \"DEP_DEL15\" #binary departure delay indicator\n",
    "TARGET_REG = \"DEP_DELAY_NEW\" #continuous departure delay (min)\n",
    "CLUSTER_COL = \"cluster\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# keep only needed columns, drop rows with missing in features/targets\n",
    "df = df.dropna(subset=FEATURE_COLS + [TARGET_CLF, TARGET_REG])\n",
    "\n",
    "X_clf = df[FEATURE_COLS]\n",
    "y_clf = df[TARGET_CLF].astype(int)\n",
    "\n",
    "X_reg = df[FEATURE_COLS]\n",
    "y_reg = df[TARGET_REG].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b93fae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>QUARTER</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>MKT_CARRIER_AIRLINE_ID</th>\n",
       "      <th>ORIGIN_AIRPORT_ID</th>\n",
       "      <th>ORIGIN_AIRPORT_SEQ_ID</th>\n",
       "      <th>ORIGIN_CITY_NAME</th>\n",
       "      <th>DEST_AIRPORT_ID</th>\n",
       "      <th>...</th>\n",
       "      <th>prev_real_delay</th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>origin_flights_day</th>\n",
       "      <th>origin_bucket</th>\n",
       "      <th>dest_flights_day</th>\n",
       "      <th>destination_bucket</th>\n",
       "      <th>distance_bucket</th>\n",
       "      <th>airline_bucket</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19393</td>\n",
       "      <td>10140</td>\n",
       "      <td>1014005</td>\n",
       "      <td>Albuquerque, NM</td>\n",
       "      <td>10423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>241</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19393</td>\n",
       "      <td>10140</td>\n",
       "      <td>1014005</td>\n",
       "      <td>Albuquerque, NM</td>\n",
       "      <td>10423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>241</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19393</td>\n",
       "      <td>10140</td>\n",
       "      <td>1014005</td>\n",
       "      <td>Albuquerque, NM</td>\n",
       "      <td>10800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19393</td>\n",
       "      <td>10140</td>\n",
       "      <td>1014005</td>\n",
       "      <td>Albuquerque, NM</td>\n",
       "      <td>10821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>265</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19393</td>\n",
       "      <td>10140</td>\n",
       "      <td>1014005</td>\n",
       "      <td>Albuquerque, NM</td>\n",
       "      <td>11259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>214</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR  QUARTER  MONTH  DAY  DAY_OF_WEEK  MKT_CARRIER_AIRLINE_ID  \\\n",
       "0  2024        1      1    1            1                   19393   \n",
       "1  2024        1      1    1            1                   19393   \n",
       "2  2024        1      1    1            1                   19393   \n",
       "3  2024        1      1    1            1                   19393   \n",
       "4  2024        1      1    1            1                   19393   \n",
       "\n",
       "   ORIGIN_AIRPORT_ID  ORIGIN_AIRPORT_SEQ_ID ORIGIN_CITY_NAME  DEST_AIRPORT_ID  \\\n",
       "0              10140                1014005  Albuquerque, NM            10423   \n",
       "1              10140                1014005  Albuquerque, NM            10423   \n",
       "2              10140                1014005  Albuquerque, NM            10800   \n",
       "3              10140                1014005  Albuquerque, NM            10821   \n",
       "4              10140                1014005  Albuquerque, NM            11259   \n",
       "\n",
       "   ...  prev_real_delay     FL_DATE  origin_flights_day  origin_bucket  \\\n",
       "0  ...              0.0  2024-01-01                  67              1   \n",
       "1  ...              0.0  2024-01-01                  67              1   \n",
       "2  ...              0.0  2024-01-01                  67              1   \n",
       "3  ...              0.0  2024-01-01                  67              1   \n",
       "4  ...              0.0  2024-01-01                  67              1   \n",
       "\n",
       "   dest_flights_day  destination_bucket  distance_bucket  airline_bucket  \\\n",
       "0               241                   2                2               1   \n",
       "1               241                   2                2               1   \n",
       "2                90                   1                3               1   \n",
       "3               265                   2                4               1   \n",
       "4               214                   2                2               1   \n",
       "\n",
       "   HOUR  cluster  \n",
       "0     7        0  \n",
       "1    18        0  \n",
       "2    14        1  \n",
       "3    15        0  \n",
       "4     5        0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "024353c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_models():\n",
    "    return {\n",
    "        \"LR_L2\": LogisticRegression(\n",
    "            random_state=0, solver=\"liblinear\", max_iter=200\n",
    "        ),\n",
    "        \"LR_L1\": LogisticRegression(\n",
    "            random_state=0,\n",
    "            penalty=\"l1\",\n",
    "            solver=\"liblinear\",\n",
    "            class_weight=\"balanced\",\n",
    "            max_iter=500,\n",
    "        ),\n",
    "        \"CART\": DecisionTreeClassifier(\n",
    "            random_state=0, class_weight=\"balanced\"\n",
    "        ),\n",
    "        \"RF\": RandomForestClassifier(\n",
    "            random_state=0, class_weight=\"balanced\", n_jobs=-1,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "def init_all_models():\n",
    "    model_names = (\"LR_L2\", \"LR_L1\", \"CART\", \"RF\")\n",
    "    techniques = (\"Baseline\", \"Scaling\")  # subset of full Lab 6 list\n",
    "\n",
    "    idx = pd.MultiIndex.from_product(\n",
    "        [model_names, techniques],\n",
    "        names=(\"model\", \"technique\"),\n",
    "    )\n",
    "    all_models = pd.DataFrame(\n",
    "        index=idx,\n",
    "        columns=[\"Precision\", \"Recall\", \"Score\", \"Model\"],\n",
    "    )\n",
    "    all_models[[\"Precision\", \"Recall\", \"Score\"]] = all_models[\n",
    "        [\"Precision\", \"Recall\", \"Score\"]\n",
    "    ].astype(float)\n",
    "    return all_models\n",
    "\n",
    "\n",
    "def standardize_data(X_train, X_out):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    Xtr = pd.DataFrame(\n",
    "        scaler.transform(X_train),\n",
    "        index=X_train.index,\n",
    "        columns=X_train.columns,\n",
    "    )\n",
    "    Xout = pd.DataFrame(\n",
    "        scaler.transform(X_out),\n",
    "        index=X_out.index,\n",
    "        columns=X_out.columns,\n",
    "    )\n",
    "    return Xtr, Xout, scaler\n",
    "\n",
    "\n",
    "def fit_and_score_model(all_models, stage_name,\n",
    "                        X_train, X_out, y_train, y_out):\n",
    "    models_dict = make_models()\n",
    "\n",
    "    for model_name, model in models_dict.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_out)\n",
    "\n",
    "        p = precision_score(y_out, y_pred)\n",
    "        r = recall_score(y_out, y_pred)\n",
    "        s = 0.5 * (p + r)\n",
    "\n",
    "        idx = (model_name, stage_name)\n",
    "        \n",
    "        all_models.at[idx, \"Precision\"] = p\n",
    "        all_models.at[idx, \"Recall\"] = r\n",
    "        all_models.at[idx, \"Score\"] = s\n",
    "        all_models.at[idx, \"Model\"] = model\n",
    "\n",
    "    return all_models\n",
    "\n",
    "\n",
    "def compare_models(all_models, technique_name=\"Scaling\"):\n",
    "    diffs = (\n",
    "        all_models.xs(technique_name, level=\"technique\").Score.values\n",
    "        - all_models.xs(\"Baseline\", level=\"technique\").Score.values\n",
    "    )\n",
    "    print(\n",
    "        f\"{technique_name}: mean ΔScore={diffs.mean():.3f}, \"\n",
    "        f\"max ΔScore={diffs.max():.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98712080",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_CLF_TRAIN_FRAC = 0.25   # 25% of global clf train\n",
    "CLUSTER_CLF_TRAIN_FRAC = 0.5    # up to 50% of each cluster's train\n",
    "GLOBAL_REG_TRAIN_FRAC = 0.25   # 25% of global reg train\n",
    "CLUSTER_REG_TRAIN_FRAC = 0.5    # 30% of each cluster's reg train\n",
    "LASSO_TRAIN_FRAC = 0.2  # 20% just for LassoCV (more expensive)\n",
    "\n",
    "MIN_SUBSAMPLE_SIZE = 10000    # don't bother subsampling below this\n",
    "\n",
    "def stratified_subsample_xy_frac(X, y, frac=1.0, random_state=0):\n",
    "    \"\"\"\n",
    "    Classification subsample: take a fraction of (X, y), stratified by y.\n",
    "    If frac >= 1.0 or dataset is already small, return (X, y) unchanged.\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    if frac >= 1.0 or n <= MIN_SUBSAMPLE_SIZE:\n",
    "        return X, y\n",
    "\n",
    "    n_sub = int(n * frac)\n",
    "    if n_sub < MIN_SUBSAMPLE_SIZE:\n",
    "        # if frac is tiny on a small dataset, just keep all\n",
    "        return X, y\n",
    "\n",
    "    X_sub, _, y_sub, _ = train_test_split(\n",
    "        X, y,\n",
    "        train_size=n_sub,\n",
    "        stratify=y,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    return X_sub, y_sub\n",
    "\n",
    "\n",
    "def random_subsample_xy_frac(X, y, frac=1.0, random_state=0):\n",
    "    \"\"\"\n",
    "    Regression subsample: take a random fraction of (X, y).\n",
    "    If frac >= 1.0 or dataset is already small, return (X, y) unchanged.\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    if frac >= 1.0 or n <= MIN_SUBSAMPLE_SIZE:\n",
    "        return X, y\n",
    "\n",
    "    n_sub = int(n * frac)\n",
    "    if n_sub < MIN_SUBSAMPLE_SIZE:\n",
    "        return X, y\n",
    "\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    idx = rng.choice(n, size=n_sub, replace=False)\n",
    "\n",
    "    if isinstance(X, (pd.DataFrame, pd.Series)):\n",
    "        return X.iloc[idx], y.iloc[idx]\n",
    "    else:\n",
    "        return X[idx], y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13d59b8",
   "metadata": {},
   "source": [
    "# Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36aba590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling: mean ΔScore=-0.000, max ΔScore=0.000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Score</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>technique</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">LR_L2</th>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.383997</td>\n",
       "      <td>0.003472</td>\n",
       "      <td>0.193734</td>\n",
       "      <td>LogisticRegression(max_iter=200, random_state=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scaling</th>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.003448</td>\n",
       "      <td>0.193214</td>\n",
       "      <td>LogisticRegression(max_iter=200, random_state=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">LR_L1</th>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.281996</td>\n",
       "      <td>0.608067</td>\n",
       "      <td>0.445032</td>\n",
       "      <td>LogisticRegression(class_weight='balanced', ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scaling</th>\n",
       "      <td>0.282005</td>\n",
       "      <td>0.608064</td>\n",
       "      <td>0.445034</td>\n",
       "      <td>LogisticRegression(class_weight='balanced', ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">CART</th>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.264188</td>\n",
       "      <td>0.390509</td>\n",
       "      <td>0.327349</td>\n",
       "      <td>DecisionTreeClassifier(class_weight='balanced'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scaling</th>\n",
       "      <td>0.264067</td>\n",
       "      <td>0.390331</td>\n",
       "      <td>0.327199</td>\n",
       "      <td>DecisionTreeClassifier(class_weight='balanced'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">RF</th>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.283857</td>\n",
       "      <td>0.296324</td>\n",
       "      <td>0.290091</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scaling</th>\n",
       "      <td>0.283797</td>\n",
       "      <td>0.296304</td>\n",
       "      <td>0.290051</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Precision    Recall     Score  \\\n",
       "model technique                                  \n",
       "LR_L2 Baseline    0.383997  0.003472  0.193734   \n",
       "      Scaling     0.382979  0.003448  0.193214   \n",
       "LR_L1 Baseline    0.281996  0.608067  0.445032   \n",
       "      Scaling     0.282005  0.608064  0.445034   \n",
       "CART  Baseline    0.264188  0.390509  0.327349   \n",
       "      Scaling     0.264067  0.390331  0.327199   \n",
       "RF    Baseline    0.283857  0.296324  0.290091   \n",
       "      Scaling     0.283797  0.296304  0.290051   \n",
       "\n",
       "                                                             Model  \n",
       "model technique                                                     \n",
       "LR_L2 Baseline   LogisticRegression(max_iter=200, random_state=...  \n",
       "      Scaling    LogisticRegression(max_iter=200, random_state=...  \n",
       "LR_L1 Baseline   LogisticRegression(class_weight='balanced', ma...  \n",
       "      Scaling    LogisticRegression(class_weight='balanced', ma...  \n",
       "CART  Baseline   DecisionTreeClassifier(class_weight='balanced'...  \n",
       "      Scaling    DecisionTreeClassifier(class_weight='balanced'...  \n",
       "RF    Baseline   (DecisionTreeClassifier(max_features='sqrt', r...  \n",
       "      Scaling    (DecisionTreeClassifier(max_features='sqrt', r...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_global_classification_models(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=0,\n",
    "    train_frac=GLOBAL_CLF_TRAIN_FRAC,\n",
    "):\n",
    "    # Full split first\n",
    "    Xtr, Xte, ytr, yte = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y,\n",
    "    )\n",
    "\n",
    "    # === SUBSAMPLE TRAINING SET (stratified by y) ===\n",
    "    Xtr_sub, ytr_sub = stratified_subsample_xy_frac(\n",
    "        Xtr, ytr,\n",
    "        frac=train_frac,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    all_models = init_all_models()\n",
    "\n",
    "    # Baseline (unscaled) – train on subsample, test on full test set\n",
    "    all_models = fit_and_score_model(\n",
    "        all_models, \"Baseline\",\n",
    "        Xtr_sub, Xte, ytr_sub, yte\n",
    "    )\n",
    "\n",
    "    # Scaling – fit scaler on subsample, transform full test\n",
    "    Xtr_s, Xte_s, scaler = standardize_data(Xtr_sub, Xte)\n",
    "    all_models = fit_and_score_model(\n",
    "        all_models, \"Scaling\",\n",
    "        Xtr_s, Xte_s, ytr_sub, yte\n",
    "    )\n",
    "\n",
    "    compare_models(all_models, \"Scaling\")\n",
    "\n",
    "    best_row = all_models.sort_values(\"Score\").iloc[-1]\n",
    "    best_model = best_row[\"Model\"]\n",
    "\n",
    "    return {\n",
    "        \"all_models\": all_models,\n",
    "        \"best_model\": best_model,\n",
    "        \"scaler\": scaler,\n",
    "        # store what we *actually used* for training\n",
    "        \"train_split\": (Xtr_sub, Xte, ytr_sub, yte),\n",
    "    }\n",
    "\n",
    "\n",
    "global_clf_results = train_global_classification_models(X_clf, y_clf)\n",
    "global_clf_results[\"all_models\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2811f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cluster_classification_models(\n",
    "    df,\n",
    "    feature_cols,\n",
    "    target_col,\n",
    "    cluster_col=\"cluster\",\n",
    "    test_size=0.2,\n",
    "    random_state=0,\n",
    "    train_frac=CLUSTER_CLF_TRAIN_FRAC,\n",
    "):\n",
    "    cluster_results = {}\n",
    "\n",
    "    for clust_id, df_c in df.groupby(cluster_col):\n",
    "        y_c = df_c[target_col].astype(int)\n",
    "        if y_c.nunique() < 2 or len(df_c) < 40:\n",
    "            continue\n",
    "\n",
    "        X_c = df_c[feature_cols]\n",
    "\n",
    "        Xtr, Xte, ytr, yte = train_test_split(\n",
    "            X_c, y_c,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "            stratify=y_c,\n",
    "        )\n",
    "\n",
    "        # === SUBSAMPLE TRAINING SET IN THIS CLUSTER ===\n",
    "        Xtr_sub, ytr_sub = stratified_subsample_xy_frac(\n",
    "            Xtr, ytr,\n",
    "            frac=train_frac,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        all_models = init_all_models()\n",
    "\n",
    "        # Baseline\n",
    "        all_models = fit_and_score_model(\n",
    "            all_models, \"Baseline\", Xtr_sub, Xte, ytr_sub, yte\n",
    "        )\n",
    "\n",
    "        # Scaling\n",
    "        Xtr_s, Xte_s, scaler = standardize_data(Xtr_sub, Xte)\n",
    "        all_models = fit_and_score_model(\n",
    "            all_models, \"Scaling\", Xtr_s, Xte_s, ytr_sub, yte\n",
    "        )\n",
    "\n",
    "        best_row = all_models.sort_values(\"Score\").iloc[-1]\n",
    "        best_model = best_row[\"Model\"]\n",
    "\n",
    "        cluster_results[clust_id] = {\n",
    "            \"all_models\": all_models,\n",
    "            \"best_model\": best_model,\n",
    "            \"scaler\": scaler,\n",
    "            \"train_split\": (Xtr_sub, Xte, ytr_sub, yte),\n",
    "        }\n",
    "\n",
    "    return cluster_results\n",
    "\n",
    "\n",
    "cluster_clf_results = train_cluster_classification_models(\n",
    "    df,\n",
    "    FEATURE_COLS,\n",
    "    TARGET_CLF,\n",
    "    cluster_col=CLUSTER_COL,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0edccf15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linear': {'model': LinearRegression(),\n",
       "  'r2': 0.013018494131079361,\n",
       "  'mse': 3005.6401227349497},\n",
       " 'lasso': {'model': LassoCV(cv=3, n_jobs=-1, random_state=0),\n",
       "  'scaler': StandardScaler(),\n",
       "  'r2': 0.013005703102057287,\n",
       "  'mse': 3005.6790750656787},\n",
       " 'train_split': (         MONTH  HOUR  origin_flights_day  airline_bucket  origin_bucket  \\\n",
       "  656565       2    16                 115               1              1   \n",
       "  5538059      9    13                  82               1              1   \n",
       "  5469672      9     6                 391               0              3   \n",
       "  3576367      6    20                 494               1              3   \n",
       "  95935        1    10                 234               0              2   \n",
       "  ...        ...   ...                 ...             ...            ...   \n",
       "  3111319      6    14                 193               1              2   \n",
       "  4536074      8    12                  24               1              1   \n",
       "  7307663     12    19                 750               1              4   \n",
       "  7136540     12    12                 469               1              3   \n",
       "  4217852      7     7                1036               1              4   \n",
       "  \n",
       "           destination_bucket  lagged_delay_flag  prev_real_delay  \n",
       "  656565                    4                  0             15.0  \n",
       "  5538059                   4                  0              0.0  \n",
       "  5469672                   3                  0              0.0  \n",
       "  3576367                   3                  0              0.0  \n",
       "  95935                     2                  0              0.0  \n",
       "  ...                     ...                ...              ...  \n",
       "  3111319                   2                  0              0.0  \n",
       "  4536074                   4                  0              0.0  \n",
       "  7307663                   1                  0              0.0  \n",
       "  7136540                   3                  0              0.0  \n",
       "  4217852                   4                  0              0.0  \n",
       "  \n",
       "  [1488816 rows x 8 columns],\n",
       "           MONTH  HOUR  origin_flights_day  airline_bucket  origin_bucket  \\\n",
       "  4154393      7     9                 494               1              3   \n",
       "  4902533      8    16                 981               1              4   \n",
       "  649961       2    17                 188               1              2   \n",
       "  489667       1     9                 125               1              2   \n",
       "  6209768     11    14                 886               1              4   \n",
       "  ...        ...   ...                 ...             ...            ...   \n",
       "  2005209      4    21                 344               1              3   \n",
       "  4582310      8    10                  13               0              1   \n",
       "  1673490      3     9                 894               1              4   \n",
       "  3544446      6    10                 350               1              3   \n",
       "  6635196     11    22                1063               1              4   \n",
       "  \n",
       "           destination_bucket  lagged_delay_flag  prev_real_delay  \n",
       "  4154393                   4                  0              0.0  \n",
       "  4902533                   1                  0             83.0  \n",
       "  649961                    3                  0              0.0  \n",
       "  489667                    3                  0              0.0  \n",
       "  6209768                   3                  0              0.0  \n",
       "  ...                     ...                ...              ...  \n",
       "  2005209                   1                  1            104.0  \n",
       "  4582310                   1                  0              0.0  \n",
       "  1673490                   2                  0              0.0  \n",
       "  3544446                   1                  0              0.0  \n",
       "  6635196                   2                  0              0.0  \n",
       "  \n",
       "  [1488816 rows x 8 columns],\n",
       "  656565       0.0\n",
       "  5538059      0.0\n",
       "  5469672      0.0\n",
       "  3576367      0.0\n",
       "  95935        0.0\n",
       "             ...  \n",
       "  3111319    195.0\n",
       "  4536074      0.0\n",
       "  7307663      0.0\n",
       "  7136540      4.0\n",
       "  4217852      0.0\n",
       "  Name: DEP_DELAY_NEW, Length: 1488816, dtype: float64,\n",
       "  4154393      1.0\n",
       "  4902533      0.0\n",
       "  649961       0.0\n",
       "  489667       0.0\n",
       "  6209768      0.0\n",
       "             ...  \n",
       "  2005209      0.0\n",
       "  4582310    255.0\n",
       "  1673490      0.0\n",
       "  3544446      0.0\n",
       "  6635196     17.0\n",
       "  Name: DEP_DELAY_NEW, Length: 1488816, dtype: float64)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_global_regression_models(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=0,\n",
    "    train_frac=GLOBAL_REG_TRAIN_FRAC,\n",
    "    lasso_frac=LASSO_TRAIN_FRAC,\n",
    "):\n",
    "    # Full split\n",
    "    Xtr, Xte, ytr, yte = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    # === SUBSAMPLE TRAINING FOR LINEAR & LASSO ===\n",
    "    Xtr_sub, ytr_sub = random_subsample_xy_frac(\n",
    "        Xtr, ytr,\n",
    "        frac=train_frac,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    # Plain linear regression (unscaled)\n",
    "    lin = LinearRegression()\n",
    "    lin.fit(Xtr_sub, ytr_sub)\n",
    "    yhat_lin = lin.predict(Xte)\n",
    "    lin_r2 = r2_score(yte, yhat_lin)\n",
    "    lin_mse = mean_squared_error(yte, yhat_lin)\n",
    "\n",
    "    # LassoCV (scaled) – optionally even smaller subsample\n",
    "    Xtr_lasso, ytr_lasso = random_subsample_xy_frac(\n",
    "        Xtr_sub, ytr_sub,\n",
    "        frac=lasso_frac / max(train_frac, 1e-9) if lasso_frac < train_frac else 1.0,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    Xtr_s, Xte_s, scaler = standardize_data(Xtr_lasso, Xte)\n",
    "    lasso = LassoCV(\n",
    "        cv=3,               # lighter than 5-fold\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    lasso.fit(Xtr_s, ytr_lasso)\n",
    "    yhat_lasso = lasso.predict(Xte_s)\n",
    "    lasso_r2 = r2_score(yte, yhat_lasso)\n",
    "    lasso_mse = mean_squared_error(yte, yhat_lasso)\n",
    "\n",
    "    return {\n",
    "        \"linear\": {\n",
    "            \"model\": lin,\n",
    "            \"r2\": lin_r2,\n",
    "            \"mse\": lin_mse,\n",
    "        },\n",
    "        \"lasso\": {\n",
    "            \"model\": lasso,\n",
    "            \"scaler\": scaler,\n",
    "            \"r2\": lasso_r2,\n",
    "            \"mse\": lasso_mse,\n",
    "        },\n",
    "        \"train_split\": (Xtr_sub, Xte, ytr_sub, yte),\n",
    "    }\n",
    "\n",
    "\n",
    "global_reg_results = train_global_regression_models(X_reg, y_reg)\n",
    "global_reg_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3860af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cluster_regression_models(\n",
    "    df,\n",
    "    feature_cols,\n",
    "    target_col,\n",
    "    cluster_col=\"cluster\",\n",
    "    test_size=0.2,\n",
    "    random_state=0,\n",
    "    min_rows=40,\n",
    "    train_frac=CLUSTER_REG_TRAIN_FRAC,\n",
    "    lasso_frac=LASSO_TRAIN_FRAC,\n",
    "):\n",
    "    cluster_reg_results = {}\n",
    "\n",
    "    for clust_id, df_c in df.groupby(cluster_col):\n",
    "        if len(df_c) < min_rows:\n",
    "            continue\n",
    "\n",
    "        X_c = df_c[feature_cols]\n",
    "        y_c = df_c[target_col].astype(float)\n",
    "\n",
    "        Xtr, Xte, ytr, yte = train_test_split(\n",
    "            X_c, y_c,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        # === SUBSAMPLE TRAINING FOR THIS CLUSTER ===\n",
    "        Xtr_sub, ytr_sub = random_subsample_xy_frac(\n",
    "            Xtr, ytr,\n",
    "            frac=train_frac,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        # Linear\n",
    "        lin = LinearRegression()\n",
    "        lin.fit(Xtr_sub, ytr_sub)\n",
    "        yhat_lin = lin.predict(Xte)\n",
    "        lin_r2 = r2_score(yte, yhat_lin)\n",
    "        lin_mse = mean_squared_error(yte, yhat_lin)\n",
    "\n",
    "        # Lasso\n",
    "        Xtr_lasso, ytr_lasso = random_subsample_xy_frac(\n",
    "            Xtr_sub, ytr_sub,\n",
    "            frac=lasso_frac / max(train_frac, 1e-9) if lasso_frac < train_frac else 1.0,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        Xtr_s, Xte_s, scaler = standardize_data(Xtr_lasso, Xte)\n",
    "        lasso = LassoCV(\n",
    "            cv=3,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        lasso.fit(Xtr_s, ytr_lasso)\n",
    "        yhat_lasso = lasso.predict(Xte_s)\n",
    "        lasso_r2 = r2_score(yte, yhat_lasso)\n",
    "        lasso_mse = mean_squared_error(yte, yhat_lasso)\n",
    "\n",
    "        cluster_reg_results[clust_id] = {\n",
    "            \"linear\": {\n",
    "                \"model\": lin,\n",
    "                \"r2\": lin_r2,\n",
    "                \"mse\": lin_mse,\n",
    "            },\n",
    "            \"lasso\": {\n",
    "                \"model\": lasso,\n",
    "                \"scaler\": scaler,\n",
    "                \"r2\": lasso_r2,\n",
    "                \"mse\": lasso_mse,\n",
    "            },\n",
    "            \"train_split\": (Xtr_sub, Xte, ytr_sub, yte),\n",
    "        }\n",
    "\n",
    "    return cluster_reg_results\n",
    "\n",
    "\n",
    "cluster_reg_results = train_cluster_regression_models(\n",
    "    df,\n",
    "    FEATURE_COLS,\n",
    "    TARGET_REG,\n",
    "    cluster_col=CLUSTER_COL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ed1dc2",
   "metadata": {},
   "source": [
    "## Not on a subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4352a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling: mean ΔScore=-0.000, max ΔScore=0.000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Score</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>technique</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">LR_L2</th>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.383242</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>0.193359</td>\n",
       "      <td>LogisticRegression(max_iter=200, random_state=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scaling</th>\n",
       "      <td>0.382331</td>\n",
       "      <td>0.003445</td>\n",
       "      <td>0.192888</td>\n",
       "      <td>LogisticRegression(max_iter=200, random_state=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">LR_L1</th>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.281975</td>\n",
       "      <td>0.608097</td>\n",
       "      <td>0.445036</td>\n",
       "      <td>LogisticRegression(class_weight='balanced', ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scaling</th>\n",
       "      <td>0.281999</td>\n",
       "      <td>0.608103</td>\n",
       "      <td>0.445051</td>\n",
       "      <td>LogisticRegression(class_weight='balanced', ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">CART</th>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.284408</td>\n",
       "      <td>0.453031</td>\n",
       "      <td>0.368720</td>\n",
       "      <td>DecisionTreeClassifier(class_weight='balanced'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scaling</th>\n",
       "      <td>0.284351</td>\n",
       "      <td>0.453001</td>\n",
       "      <td>0.368676</td>\n",
       "      <td>DecisionTreeClassifier(class_weight='balanced'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">RF</th>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.299683</td>\n",
       "      <td>0.378473</td>\n",
       "      <td>0.339078</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scaling</th>\n",
       "      <td>0.299631</td>\n",
       "      <td>0.378390</td>\n",
       "      <td>0.339011</td>\n",
       "      <td>(DecisionTreeClassifier(max_features='sqrt', r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Precision    Recall     Score  \\\n",
       "model technique                                  \n",
       "LR_L2 Baseline    0.383242  0.003475  0.193359   \n",
       "      Scaling     0.382331  0.003445  0.192888   \n",
       "LR_L1 Baseline    0.281975  0.608097  0.445036   \n",
       "      Scaling     0.281999  0.608103  0.445051   \n",
       "CART  Baseline    0.284408  0.453031  0.368720   \n",
       "      Scaling     0.284351  0.453001  0.368676   \n",
       "RF    Baseline    0.299683  0.378473  0.339078   \n",
       "      Scaling     0.299631  0.378390  0.339011   \n",
       "\n",
       "                                                             Model  \n",
       "model technique                                                     \n",
       "LR_L2 Baseline   LogisticRegression(max_iter=200, random_state=...  \n",
       "      Scaling    LogisticRegression(max_iter=200, random_state=...  \n",
       "LR_L1 Baseline   LogisticRegression(class_weight='balanced', ma...  \n",
       "      Scaling    LogisticRegression(class_weight='balanced', ma...  \n",
       "CART  Baseline   DecisionTreeClassifier(class_weight='balanced'...  \n",
       "      Scaling    DecisionTreeClassifier(class_weight='balanced'...  \n",
       "RF    Baseline   (DecisionTreeClassifier(max_features='sqrt', r...  \n",
       "      Scaling    (DecisionTreeClassifier(max_features='sqrt', r...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def train_global_classification_models(X, y, test_size=0.2, random_state=0):\n",
    "    Xtr, Xte, ytr, yte = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    all_models = init_all_models()\n",
    "\n",
    "    # Baseline\n",
    "    all_models = fit_and_score_model(\n",
    "        all_models, \"Baseline\", Xtr, Xte, ytr, yte\n",
    "    )\n",
    "\n",
    "    # Scaling\n",
    "    Xtr_s, Xte_s, scaler = standardize_data(Xtr, Xte)\n",
    "    all_models = fit_and_score_model(\n",
    "        all_models, \"Scaling\", Xtr_s, Xte_s, ytr, yte\n",
    "    )\n",
    "\n",
    "    compare_models(all_models, \"Scaling\")\n",
    "\n",
    "    best_row = all_models.sort_values(\"Score\").iloc[-1]\n",
    "    best_model = best_row[\"Model\"]\n",
    "\n",
    "    return {\n",
    "        \"all_models\": all_models,\n",
    "        \"best_model\": best_model,\n",
    "        \"scaler\": scaler,\n",
    "        \"train_split\": (Xtr, Xte, ytr, yte),\n",
    "    }\n",
    "\n",
    "\n",
    "global_clf_results = train_global_classification_models(X_clf, y_clf)\n",
    "global_clf_results[\"all_models\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608de875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cluster_classification_models(df,\n",
    "                                        feature_cols,\n",
    "                                        target_col,\n",
    "                                        cluster_col=\"cluster\",\n",
    "                                        test_size=0.2,\n",
    "                                        random_state=0):\n",
    "    cluster_results = {}\n",
    "\n",
    "    for clust_id, df_c in df.groupby(cluster_col):\n",
    "        y_c = df_c[target_col].astype(int)\n",
    "        if y_c.nunique() < 2 or len(df_c) < 40:\n",
    "            continue\n",
    "\n",
    "        X_c = df_c[feature_cols]\n",
    "\n",
    "        Xtr, Xte, ytr, yte = train_test_split(\n",
    "            X_c, y_c,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "            stratify=y_c,\n",
    "        )\n",
    "\n",
    "        all_models = init_all_models()\n",
    "\n",
    "        all_models = fit_and_score_model(\n",
    "            all_models, \"Baseline\", Xtr, Xte, ytr, yte\n",
    "        )\n",
    "\n",
    "        Xtr_s, Xte_s, scaler = standardize_data(Xtr, Xte)\n",
    "        all_models = fit_and_score_model(\n",
    "            all_models, \"Scaling\", Xtr_s, Xte_s, ytr, yte\n",
    "        )\n",
    "\n",
    "        best_row = all_models.sort_values(\"Score\").iloc[-1]\n",
    "        best_model = best_row[\"Model\"]\n",
    "\n",
    "        cluster_results[clust_id] = {\n",
    "            \"all_models\": all_models,\n",
    "            \"best_model\": best_model,\n",
    "            \"scaler\": scaler,\n",
    "        }\n",
    "\n",
    "    return cluster_results\n",
    "\n",
    "\n",
    "cluster_clf_results = train_cluster_classification_models(\n",
    "    df,\n",
    "    FEATURE_COLS,\n",
    "    TARGET_CLF,\n",
    "    cluster_col=CLUSTER_COL,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d360e644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_global_regression_models(X, y, test_size=0.2, random_state=0):\n",
    "    Xtr, Xte, ytr, yte = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Plain linear regression (unscaled)\n",
    "    lin = LinearRegression()\n",
    "    lin.fit(Xtr, ytr)\n",
    "    yhat_lin = lin.predict(Xte)\n",
    "    lin_r2 = r2_score(yte, yhat_lin)\n",
    "    lin_mse = mean_squared_error(yte, yhat_lin)\n",
    "\n",
    "    # LassoCV (scaled)\n",
    "    Xtr_s, Xte_s, scaler = standardize_data(Xtr, Xte)\n",
    "    lasso = LassoCV(cv=5, random_state=random_state)\n",
    "    lasso.fit(Xtr_s, ytr)\n",
    "    yhat_lasso = lasso.predict(Xte_s)\n",
    "    lasso_r2 = r2_score(yte, yhat_lasso)\n",
    "    lasso_mse = mean_squared_error(yte, yhat_lasso)\n",
    "\n",
    "    return {\n",
    "        \"linear\": {\n",
    "            \"model\": lin,\n",
    "            \"r2\": lin_r2,\n",
    "            \"mse\": lin_mse,\n",
    "        },\n",
    "        \"lasso\": {\n",
    "            \"model\": lasso,\n",
    "            \"scaler\": scaler,\n",
    "            \"r2\": lasso_r2,\n",
    "            \"mse\": lasso_mse,\n",
    "        },\n",
    "        \"train_split\": (Xtr, Xte, ytr, yte),\n",
    "    }\n",
    "\n",
    "\n",
    "global_reg_results = train_global_regression_models(X_reg, y_reg)\n",
    "global_reg_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a2f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cluster_regression_models(df,\n",
    "                                    feature_cols,\n",
    "                                    target_col,\n",
    "                                    cluster_col=\"cluster\",\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=0,\n",
    "                                    min_rows=40):\n",
    "    cluster_reg_results = {}\n",
    "\n",
    "    for clust_id, df_c in df.groupby(cluster_col):\n",
    "        if len(df_c) < min_rows:\n",
    "            continue\n",
    "\n",
    "        X_c = df_c[feature_cols]\n",
    "        y_c = df_c[target_col].astype(float)\n",
    "\n",
    "        Xtr, Xte, ytr, yte = train_test_split(\n",
    "            X_c, y_c,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        # linear\n",
    "        lin = LinearRegression()\n",
    "        lin.fit(Xtr, ytr)\n",
    "        yhat_lin = lin.predict(Xte)\n",
    "        lin_r2 = r2_score(yte, yhat_lin)\n",
    "        lin_mse = mean_squared_error(yte, yhat_lin)\n",
    "\n",
    "        # lasso (scaled)\n",
    "        Xtr_s, Xte_s, scaler = standardize_data(Xtr, Xte)\n",
    "        lasso = LassoCV(cv=5, random_state=random_state)\n",
    "        lasso.fit(Xtr_s, ytr)\n",
    "        yhat_lasso = lasso.predict(Xte_s)\n",
    "        lasso_r2 = r2_score(yte, yhat_lasso)\n",
    "        lasso_mse = mean_squared_error(yte, yhat_lasso)\n",
    "\n",
    "        cluster_reg_results[clust_id] = {\n",
    "            \"linear\": {\n",
    "                \"model\": lin,\n",
    "                \"r2\": lin_r2,\n",
    "                \"mse\": lin_mse,\n",
    "            },\n",
    "            \"lasso\": {\n",
    "                \"model\": lasso,\n",
    "                \"scaler\": scaler,\n",
    "                \"r2\": lasso_r2,\n",
    "                \"mse\": lasso_mse,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    return cluster_reg_results\n",
    "\n",
    "\n",
    "cluster_reg_results = train_cluster_regression_models(\n",
    "    df,\n",
    "    FEATURE_COLS,\n",
    "    TARGET_REG,\n",
    "    cluster_col=CLUSTER_COL,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1bc1a7",
   "metadata": {},
   "source": [
    "# Cluster Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a49af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters(df, k, cluster_features, cluster_col_name):\n",
    "    X_cluster = df[cluster_features].copy()\n",
    "\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    labels = kmeans.fit_predict(X_cluster)\n",
    "\n",
    "    df = df.copy()\n",
    "    df[cluster_col_name] = labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ead0d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_EXPERIMENTS = [\n",
    "    {\n",
    "        \"name\": \"k3_all_feats\",\n",
    "        \"k\": 3,\n",
    "        \"cluster_features\": FEATURE_COLS,      # use all current features\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"k4_all_feats\",\n",
    "        \"k\": 4,\n",
    "        \"cluster_features\": FEATURE_COLS,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"k5_all_feats\",\n",
    "        \"k\": 5,\n",
    "        \"cluster_features\": FEATURE_COLS,\n",
    "    },\n",
    "    # add more as needed\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f5aca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cluster_clf_results = {}\n",
    "all_cluster_reg_results = {}\n",
    "\n",
    "for cfg in CLUSTER_EXPERIMENTS:\n",
    "    exp_name = cfg[\"name\"]\n",
    "    k = cfg[\"k\"]\n",
    "    cluster_features = cfg[\"cluster_features\"]\n",
    "\n",
    "    # 1) assign cluster labels for this experiment\n",
    "    cluster_col = f\"cluster_{exp_name}\"\n",
    "    df_with_clusters = assign_clusters(df, k, cluster_features, cluster_col)\n",
    "\n",
    "    # 2) train cluster-specific classification models\n",
    "    cluster_clf_results = train_cluster_classification_models(\n",
    "        df_with_clusters,\n",
    "        FEATURE_COLS,       # you can choose to change this too, if needed\n",
    "        TARGET_CLF,\n",
    "        cluster_col=cluster_col,\n",
    "    )\n",
    "\n",
    "    # 3) train cluster-specific regression models\n",
    "    cluster_reg_results = train_cluster_regression_models(\n",
    "        df_with_clusters,\n",
    "        FEATURE_COLS,\n",
    "        TARGET_REG,\n",
    "        cluster_col=cluster_col,\n",
    "    )\n",
    "\n",
    "    # 4) store results keyed by experiment name\n",
    "    all_cluster_clf_results[exp_name] = cluster_clf_results\n",
    "    all_cluster_reg_results[exp_name] = cluster_reg_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea8ff41",
   "metadata": {},
   "source": [
    "# Gathering Summary Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dd3e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_rows = []\n",
    "#global classification\n",
    "global_df = global_clf_results[\"all_models\"].copy()\n",
    "global_df = global_df.reset_index()\n",
    "for _, row in global_df.iterrows():\n",
    "    summary_rows.append({\n",
    "        \"Level\": \"Global\",\n",
    "        \"Cluster\": \"-\",\n",
    "        \"ClusterExp\": \"-\",  \n",
    "        \"Task\": \"Classification\",\n",
    "        \"Model\": row[\"model\"],\n",
    "        \"Technique\": row[\"technique\"],\n",
    "        \"Precision\": row[\"Precision\"],\n",
    "        \"Recall\": row[\"Recall\"],\n",
    "        \"Score\": row[\"Score\"],\n",
    "        \"R2\": None,\n",
    "        \"MSE\": None\n",
    "    })\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# CLUSTER CLASSIFICATION (MULTIPLE EXPERIMENTS)\n",
    "# ==========================================================\n",
    "for clust_id, res in cluster_clf_results.items():\n",
    "    df_m = res[\"all_models\"].reset_index()  # bring model, technique out of index\n",
    "\n",
    "    for _, row in df_m.iterrows():\n",
    "        summary_rows.append({\n",
    "            \"Level\": \"Cluster\",\n",
    "            \"Cluster\": clust_id,\n",
    "            \"ClusterExp\": exp_name,\n",
    "            \"Task\": \"Classification\",\n",
    "            \"Model\": row[\"model\"],          # now from columns\n",
    "            \"Technique\": row[\"technique\"],  # Baseline / Scaling\n",
    "            \"Precision\": row[\"Precision\"],\n",
    "            \"Recall\": row[\"Recall\"],\n",
    "            \"Score\": row[\"Score\"],\n",
    "            \"R2\": None,\n",
    "            \"MSE\": None,\n",
    "        })\n",
    "\n",
    "# --------------------------\n",
    "# GLOBAL REGRESSION\n",
    "# --------------------------\n",
    "# linear\n",
    "summary_rows.append({\n",
    "    \"Level\": \"Global\",\n",
    "    \"Cluster\": \"-\",\n",
    "    \"ClusterExp\": \"-\",\n",
    "    \"Task\": \"Regression\",\n",
    "    \"Model\": \"LinearRegression\",\n",
    "    \"Technique\": \"Baseline\",\n",
    "    \"Precision\": None,\n",
    "    \"Recall\": None,\n",
    "    \"Score\": None,\n",
    "    \"R2\": global_reg_results[\"linear\"][\"r2\"],\n",
    "    \"MSE\": global_reg_results[\"linear\"][\"mse\"]\n",
    "})\n",
    "\n",
    "# lasso\n",
    "summary_rows.append({\n",
    "    \"Level\": \"Global\",\n",
    "    \"Cluster\": \"-\",\n",
    "    \"ClusterExp\": \"-\",\n",
    "    \"Task\": \"Regression\",\n",
    "    \"Model\": \"LassoCV\",\n",
    "    \"Technique\": \"Scaled\",\n",
    "    \"Precision\": None,\n",
    "    \"Recall\": None,\n",
    "    \"Score\": None,\n",
    "    \"R2\": global_reg_results[\"lasso\"][\"r2\"],\n",
    "    \"MSE\": global_reg_results[\"lasso\"][\"mse\"]\n",
    "})\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# CLUSTER REGRESSION (MULTIPLE EXPERIMENTS)\n",
    "# ==========================================================\n",
    "for exp_name, cluster_reg_results in all_cluster_reg_results.items():\n",
    "    for clust_id, res in cluster_reg_results.items():\n",
    "\n",
    "        # linear\n",
    "        summary_rows.append({\n",
    "            \"Level\": \"Cluster\",\n",
    "            \"Cluster\": clust_id,\n",
    "            \"ClusterExp\": exp_name, \n",
    "            \"Task\": \"Regression\",\n",
    "            \"Model\": \"LinearRegression\",\n",
    "            \"Technique\": \"Baseline\",\n",
    "            \"Precision\": None,\n",
    "            \"Recall\": None,\n",
    "            \"Score\": None,\n",
    "            \"R2\": res[\"linear\"][\"r2\"],\n",
    "            \"MSE\": res[\"linear\"][\"mse\"]\n",
    "        })\n",
    "\n",
    "        # lasso\n",
    "        summary_rows.append({\n",
    "            \"Level\": \"Cluster\",\n",
    "            \"Cluster\": clust_id,\n",
    "            \"ClusterExp\": exp_name,\n",
    "            \"Task\": \"Regression\",\n",
    "            \"Model\": \"LassoCV\",\n",
    "            \"Technique\": \"Scaled\",\n",
    "            \"Precision\": None,\n",
    "            \"Recall\": None,\n",
    "            \"Score\": None,\n",
    "            \"R2\": res[\"lasso\"][\"r2\"],\n",
    "            \"MSE\": res[\"lasso\"][\"mse\"]\n",
    "        })\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# BUILD FINAL SUMMARY TABLE\n",
    "# --------------------------\n",
    "summary_table = pd.DataFrame(summary_rows)\n",
    "\n",
    "# sorting for readability\n",
    "summary_table = summary_table.sort_values(\n",
    "    by=[\"Task\", \"ClusterExp\", \"Level\", \"Cluster\", \"Model\"]\n",
    ").reset_index(drop=True)\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bad2cd",
   "metadata": {},
   "source": [
    "# More complex approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011332f1",
   "metadata": {},
   "source": [
    "## Helper functions that draw a subset of the total dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2816d388",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84275b0",
   "metadata": {},
   "source": [
    "### HistGradientBoostingClassifier on Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad4335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global classification split (training already subsampled by your function)\n",
    "Xtr_clf, Xte_clf, ytr_clf, yte_clf = global_clf_results[\"train_split\"]\n",
    "\n",
    "# Global regression split (training already subsampled)\n",
    "Xtr_reg, Xte_reg, ytr_reg, yte_reg = global_reg_results[\"train_split\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee55d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hgb_classifier_on_existing_split(\n",
    "    Xtr, Xte, ytr, yte,\n",
    "    random_state=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train HistGradientBoostingClassifier on the (already subsampled) train split.\n",
    "    \"\"\"\n",
    "    hgb_clf = HistGradientBoostingClassifier(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        max_iter=200,\n",
    "        max_leaf_nodes=31,\n",
    "        l2_regularization=1.0,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    hgb_clf.fit(Xtr, ytr)\n",
    "    y_pred = hgb_clf.predict(Xte)\n",
    "\n",
    "    p = precision_score(yte, y_pred)\n",
    "    r = recall_score(yte, y_pred)\n",
    "    s = 0.5 * (p + r)\n",
    "\n",
    "    return {\n",
    "        \"model\": hgb_clf,\n",
    "        \"precision\": p,\n",
    "        \"recall\": r,\n",
    "        \"score\": s,\n",
    "        \"n_train_used\": len(Xtr),\n",
    "    }\n",
    "\n",
    "\n",
    "hgb_clf_results = train_hgb_classifier_on_existing_split(\n",
    "    Xtr_clf, Xte_clf, ytr_clf, yte_clf,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "print(\"HGB classifier – Precision: {:.3f}, Recall: {:.3f}, Score: {:.3f}\".format(\n",
    "    hgb_clf_results[\"precision\"],\n",
    "    hgb_clf_results[\"recall\"],\n",
    "    hgb_clf_results[\"score\"],\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6ad6ef",
   "metadata": {},
   "source": [
    "### Stacking ensemble (LogReg + RF, global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b92826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stacking_classifier_on_existing_split(\n",
    "    Xtr, Xte, ytr, yte,\n",
    "    random_state=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train stacking classifier (LogisticRegression + RandomForest) on the\n",
    "    subsampled global train split.\n",
    "    \"\"\"\n",
    "    # Base estimators\n",
    "    base_lr = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        LogisticRegression(\n",
    "            random_state=random_state,\n",
    "            solver=\"liblinear\",\n",
    "            max_iter=300,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    base_rf = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_leaf=20,\n",
    "        max_features=\"sqrt\",\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    stack_clf = StackingClassifier(\n",
    "        estimators=[\n",
    "            (\"lr\", base_lr),\n",
    "            (\"rf\", base_rf),\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(\n",
    "            random_state=random_state,\n",
    "            solver=\"lbfgs\",\n",
    "            max_iter=300,\n",
    "        ),\n",
    "        passthrough=True,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    stack_clf.fit(Xtr, ytr)\n",
    "    y_pred = stack_clf.predict(Xte)\n",
    "\n",
    "    p = precision_score(yte, y_pred)\n",
    "    r = recall_score(yte, y_pred)\n",
    "    s = 0.5 * (p + r)\n",
    "\n",
    "    return {\n",
    "        \"model\": stack_clf,\n",
    "        \"precision\": p,\n",
    "        \"recall\": r,\n",
    "        \"score\": s,\n",
    "        \"n_train_used\": len(Xtr),\n",
    "    }\n",
    "\n",
    "\n",
    "stack_clf_results = train_stacking_classifier_on_existing_split(\n",
    "    Xtr_clf, Xte_clf, ytr_clf, yte_clf,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "print(\"Stacking classifier – Precision: {:.3f}, Recall: {:.3f}, Score: {:.3f}\".format(\n",
    "    stack_clf_results[\"precision\"],\n",
    "    stack_clf_results[\"recall\"],\n",
    "    stack_clf_results[\"score\"],\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9514f652",
   "metadata": {},
   "source": [
    "### HistGradientBoostingClassifier on Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de2e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cluster_hgb_classifiers(\n",
    "    cluster_clf_results,\n",
    "    random_state=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    For each cluster in cluster_clf_results, train a HistGradientBoostingClassifier\n",
    "    on that cluster's (already-subsampled) train split and evaluate on its test split.\n",
    "    \n",
    "    Assumes each entry has:\n",
    "        res[\"train_split\"] = (Xtr_sub, Xte, ytr_sub, yte)\n",
    "    \"\"\"\n",
    "    cluster_hgb_clf_results = {}\n",
    "\n",
    "    for clust_id, res in cluster_clf_results.items():\n",
    "        if \"train_split\" not in res:\n",
    "            # you can raise or skip; I'm being defensive\n",
    "            continue\n",
    "\n",
    "        Xtr_c, Xte_c, ytr_c, yte_c = res[\"train_split\"]\n",
    "\n",
    "        hgb_clf = HistGradientBoostingClassifier(\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            max_iter=200,\n",
    "            max_leaf_nodes=31,\n",
    "            l2_regularization=1.0,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        hgb_clf.fit(Xtr_c, ytr_c)\n",
    "        y_pred_c = hgb_clf.predict(Xte_c)\n",
    "\n",
    "        p = precision_score(yte_c, y_pred_c)\n",
    "        r = recall_score(yte_c, y_pred_c)\n",
    "        s = 0.5 * (p + r)\n",
    "\n",
    "        cluster_hgb_clf_results[clust_id] = {\n",
    "            \"model\": hgb_clf,\n",
    "            \"precision\": p,\n",
    "            \"recall\": r,\n",
    "            \"score\": s,\n",
    "            \"n_train_used\": len(Xtr_c),\n",
    "        }\n",
    "\n",
    "    return cluster_hgb_clf_results\n",
    "\n",
    "\n",
    "cluster_hgb_clf_results = train_cluster_hgb_classifiers(cluster_clf_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d62142",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_hgb_clf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e1d2d6",
   "metadata": {},
   "source": [
    "### Clustered Stacking (LogReg + RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fd5d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cluster_stacking_classifiers(\n",
    "    cluster_clf_results,\n",
    "    random_state=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    For each cluster, train a stacking classifier (LogReg + RF) on its train split.\n",
    "    \"\"\"\n",
    "\n",
    "    cluster_stack_clf_results = {}\n",
    "\n",
    "    for clust_id, res in cluster_clf_results.items():\n",
    "        if \"train_split\" not in res:\n",
    "            continue\n",
    "\n",
    "        Xtr_c, Xte_c, ytr_c, yte_c = res[\"train_split\"]\n",
    "\n",
    "        # Base estimators per cluster\n",
    "        base_lr = make_pipeline(\n",
    "            StandardScaler(),\n",
    "            LogisticRegression(\n",
    "                random_state=random_state,\n",
    "                solver=\"liblinear\",\n",
    "                max_iter=300,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        base_rf = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_leaf=20,\n",
    "            max_features=\"sqrt\",\n",
    "            class_weight=\"balanced\",\n",
    "            n_jobs=-1,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        stack_clf = StackingClassifier(\n",
    "            estimators=[\n",
    "                (\"lr\", base_lr),\n",
    "                (\"rf\", base_rf),\n",
    "            ],\n",
    "            final_estimator=LogisticRegression(\n",
    "                random_state=random_state,\n",
    "                solver=\"lbfgs\",\n",
    "                max_iter=300,\n",
    "            ),\n",
    "            passthrough=True,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "\n",
    "        stack_clf.fit(Xtr_c, ytr_c)\n",
    "        y_pred_c = stack_clf.predict(Xte_c)\n",
    "\n",
    "        p = precision_score(yte_c, y_pred_c)\n",
    "        r = recall_score(yte_c, y_pred_c)\n",
    "        s = 0.5 * (p + r)\n",
    "\n",
    "        cluster_stack_clf_results[clust_id] = {\n",
    "            \"model\": stack_clf,\n",
    "            \"precision\": p,\n",
    "            \"recall\": r,\n",
    "            \"score\": s,\n",
    "            \"n_train_used\": len(Xtr_c),\n",
    "        }\n",
    "\n",
    "    return cluster_stack_clf_results\n",
    "\n",
    "\n",
    "cluster_stack_clf_results = train_cluster_stacking_classifiers(cluster_clf_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb01ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_stack_clf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f7784a",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "### RandomForestRegressor on Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fc1e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf_regressor_on_existing_split(\n",
    "    Xtr, Xte, ytr, yte,\n",
    "    random_state=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train RandomForestRegressor on the subsampled global train split.\n",
    "    \"\"\"\n",
    "    rf_reg = RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        min_samples_leaf=20,\n",
    "        max_features=\"sqrt\",\n",
    "        n_jobs=-1,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    rf_reg.fit(Xtr, ytr)\n",
    "    y_pred = rf_reg.predict(Xte)\n",
    "\n",
    "    r2 = r2_score(yte, y_pred)\n",
    "    mse = mean_squared_error(yte, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"model\": rf_reg,\n",
    "        \"r2\": r2,\n",
    "        \"mse\": mse,\n",
    "        \"n_train_used\": len(Xtr),\n",
    "    }\n",
    "\n",
    "\n",
    "rf_reg_results = train_rf_regressor_on_existing_split(\n",
    "    Xtr_reg, Xte_reg, ytr_reg, yte_reg,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "print(\"RF regressor – R²: {:.3f}, MSE: {:.1f}\".format(\n",
    "    rf_reg_results[\"r2\"],\n",
    "    rf_reg_results[\"mse\"],\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb65c39f",
   "metadata": {},
   "source": [
    "### HistGradientBoostingRegressor on Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f07409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hgb_regressor_on_existing_split(\n",
    "    Xtr, Xte, ytr, yte,\n",
    "    random_state=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train HistGradientBoostingRegressor on the subsampled global train split.\n",
    "    \"\"\"\n",
    "    hgb_reg = HistGradientBoostingRegressor(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        max_iter=200,\n",
    "        max_leaf_nodes=31,\n",
    "        l2_regularization=1.0,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    hgb_reg.fit(Xtr, ytr)\n",
    "    y_pred = hgb_reg.predict(Xte)\n",
    "\n",
    "    r2 = r2_score(yte, y_pred)\n",
    "    mse = mean_squared_error(yte, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"model\": hgb_reg,\n",
    "        \"r2\": r2,\n",
    "        \"mse\": mse,\n",
    "        \"n_train_used\": len(Xtr),\n",
    "    }\n",
    "\n",
    "\n",
    "hgb_reg_results = train_hgb_regressor_on_existing_split(\n",
    "    Xtr_reg, Xte_reg, ytr_reg, yte_reg,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "print(\"HGB regressor – R²: {:.3f}, MSE: {:.1f}\".format(\n",
    "    hgb_reg_results[\"r2\"],\n",
    "    hgb_reg_results[\"mse\"],\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbab6e79",
   "metadata": {},
   "source": [
    "### Clustered RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683abba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cluster_rf_regressors(\n",
    "    cluster_reg_results,\n",
    "    random_state=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    For each cluster in cluster_reg_results, train a RandomForestRegressor\n",
    "    on its (already-subsampled) train split and evaluate on test split.\n",
    "    \"\"\"\n",
    "\n",
    "    cluster_rf_reg_results = {}\n",
    "\n",
    "    for clust_id, res in cluster_reg_results.items():\n",
    "        if \"train_split\" not in res:\n",
    "            continue\n",
    "\n",
    "        Xtr_c, Xte_c, ytr_c, yte_c = res[\"train_split\"]\n",
    "\n",
    "        rf_reg = RandomForestRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=15,\n",
    "            min_samples_leaf=20,\n",
    "            max_features=\"sqrt\",\n",
    "            n_jobs=-1,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        rf_reg.fit(Xtr_c, ytr_c)\n",
    "        y_pred_c = rf_reg.predict(Xte_c)\n",
    "\n",
    "        r2 = r2_score(yte_c, y_pred_c)\n",
    "        mse = mean_squared_error(yte_c, y_pred_c)\n",
    "\n",
    "        cluster_rf_reg_results[clust_id] = {\n",
    "            \"model\": rf_reg,\n",
    "            \"r2\": r2,\n",
    "            \"mse\": mse,\n",
    "            \"n_train_used\": len(Xtr_c),\n",
    "        }\n",
    "\n",
    "    return cluster_rf_reg_results\n",
    "\n",
    "\n",
    "cluster_rf_reg_results = train_cluster_rf_regressors(cluster_reg_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab472cf7",
   "metadata": {},
   "source": [
    "### Clustered HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bfde7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cluster_hgb_regressors(\n",
    "    cluster_reg_results,\n",
    "    random_state=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    For each cluster, train a HistGradientBoostingRegressor on its train split.\n",
    "    \"\"\"\n",
    "\n",
    "    cluster_hgb_reg_results = {}\n",
    "\n",
    "    for clust_id, res in cluster_reg_results.items():\n",
    "        if \"train_split\" not in res:\n",
    "            continue\n",
    "\n",
    "        Xtr_c, Xte_c, ytr_c, yte_c = res[\"train_split\"]\n",
    "\n",
    "        hgb_reg = HistGradientBoostingRegressor(\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            max_iter=200,\n",
    "            max_leaf_nodes=31,\n",
    "            l2_regularization=1.0,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        hgb_reg.fit(Xtr_c, ytr_c)\n",
    "        y_pred_c = hgb_reg.predict(Xte_c)\n",
    "\n",
    "        r2 = r2_score(yte_c, y_pred_c)\n",
    "        mse = mean_squared_error(yte_c, y_pred_c)\n",
    "\n",
    "        cluster_hgb_reg_results[clust_id] = {\n",
    "            \"model\": hgb_reg,\n",
    "            \"r2\": r2,\n",
    "            \"mse\": mse,\n",
    "            \"n_train_used\": len(Xtr_c),\n",
    "        }\n",
    "\n",
    "    return cluster_hgb_reg_results\n",
    "\n",
    "\n",
    "cluster_hgb_reg_results = train_cluster_hgb_regressors(cluster_reg_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39799fd",
   "metadata": {},
   "source": [
    "# Add to summary results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994fb3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_rows = []\n",
    "\n",
    "# Classification – HGB\n",
    "summary_rows.append({\n",
    "    \"Level\": \"Global\",\n",
    "    \"Cluster\": \"-\",\n",
    "    \"ClusterExp\": \"-\",\n",
    "    \"Task\": \"Classification\",\n",
    "    \"Model\": \"HistGradientBoostingClassifier\",\n",
    "    \"Technique\": \"SubsampledGlobal\",\n",
    "    \"Precision\": hgb_clf_results[\"precision\"],\n",
    "    \"Recall\": hgb_clf_results[\"recall\"],\n",
    "    \"Score\": hgb_clf_results[\"score\"],\n",
    "    \"R2\": None,\n",
    "    \"MSE\": None,\n",
    "})\n",
    "\n",
    "# Classification – Stacking\n",
    "summary_rows.append({\n",
    "    \"Level\": \"Global\",\n",
    "    \"Cluster\": \"-\",\n",
    "    \"ClusterExp\": \"-\",\n",
    "    \"Task\": \"Classification\",\n",
    "    \"Model\": \"Stacking(LR+RF)\",\n",
    "    \"Technique\": \"SubsampledGlobal\",\n",
    "    \"Precision\": stack_clf_results[\"precision\"],\n",
    "    \"Recall\": stack_clf_results[\"recall\"],\n",
    "    \"Score\": stack_clf_results[\"score\"],\n",
    "    \"R2\": None,\n",
    "    \"MSE\": None,\n",
    "})\n",
    "\n",
    "# Regression – RF\n",
    "summary_rows.append({\n",
    "    \"Level\": \"Global\",\n",
    "    \"Cluster\": \"-\",\n",
    "    \"ClusterExp\": \"-\",\n",
    "    \"Task\": \"Regression\",\n",
    "    \"Model\": \"RandomForestRegressor\",\n",
    "    \"Technique\": \"SubsampledGlobal\",\n",
    "    \"Precision\": None,\n",
    "    \"Recall\": None,\n",
    "    \"Score\": None,\n",
    "    \"R2\": rf_reg_results[\"r2\"],\n",
    "    \"MSE\": rf_reg_results[\"mse\"],\n",
    "})\n",
    "\n",
    "# Regression – HGB\n",
    "summary_rows.append({\n",
    "    \"Level\": \"Global\",\n",
    "    \"Cluster\": \"-\",\n",
    "    \"ClusterExp\": \"-\",\n",
    "    \"Task\": \"Regression\",\n",
    "    \"Model\": \"HistGradientBoostingRegressor\",\n",
    "    \"Technique\": \"SubsampledGlobal\",\n",
    "    \"Precision\": None,\n",
    "    \"Recall\": None,\n",
    "    \"Score\": None,\n",
    "    \"R2\": hgb_reg_results[\"r2\"],\n",
    "    \"MSE\": hgb_reg_results[\"mse\"],\n",
    "})\n",
    "\n",
    "# Classification – HGB per cluster\n",
    "for clust_id, res in cluster_hgb_clf_results.items():\n",
    "    summary_rows.append({\n",
    "        \"Level\": \"Cluster\",\n",
    "        \"Cluster\": clust_id,\n",
    "        \"ClusterExp\": exp_name if \"exp_name\" in locals() else \"-\",  # or however you're tracking it\n",
    "        \"Task\": \"Classification\",\n",
    "        \"Model\": \"HistGradientBoostingClassifier\",\n",
    "        \"Technique\": \"SubsampledCluster\",\n",
    "        \"Precision\": res[\"precision\"],\n",
    "        \"Recall\": res[\"recall\"],\n",
    "        \"Score\": res[\"score\"],\n",
    "        \"R2\": None,\n",
    "        \"MSE\": None,\n",
    "    })\n",
    "\n",
    "# Classification – Stacking per cluster\n",
    "for clust_id, res in cluster_stack_clf_results.items():\n",
    "    summary_rows.append({\n",
    "        \"Level\": \"Cluster\",\n",
    "        \"Cluster\": clust_id,\n",
    "        \"ClusterExp\": exp_name if \"exp_name\" in locals() else \"-\",\n",
    "        \"Task\": \"Classification\",\n",
    "        \"Model\": \"Stacking(LR+RF)\",\n",
    "        \"Technique\": \"SubsampledCluster\",\n",
    "        \"Precision\": res[\"precision\"],\n",
    "        \"Recall\": res[\"recall\"],\n",
    "        \"Score\": res[\"score\"],\n",
    "        \"R2\": None,\n",
    "        \"MSE\": None,\n",
    "    })\n",
    "\n",
    "# Regression – RF per cluster\n",
    "for clust_id, res in cluster_rf_reg_results.items():\n",
    "    summary_rows.append({\n",
    "        \"Level\": \"Cluster\",\n",
    "        \"Cluster\": clust_id,\n",
    "        \"ClusterExp\": exp_name if \"exp_name\" in locals() else \"-\",\n",
    "        \"Task\": \"Regression\",\n",
    "        \"Model\": \"RandomForestRegressor\",\n",
    "        \"Technique\": \"SubsampledCluster\",\n",
    "        \"Precision\": None,\n",
    "        \"Recall\": None,\n",
    "        \"Score\": None,\n",
    "        \"R2\": res[\"r2\"],\n",
    "        \"MSE\": res[\"mse\"],\n",
    "    })\n",
    "\n",
    "# Regression – HGB per cluster\n",
    "for clust_id, res in cluster_hgb_reg_results.items():\n",
    "    summary_rows.append({\n",
    "        \"Level\": \"Cluster\",\n",
    "        \"Cluster\": clust_id,\n",
    "        \"ClusterExp\": exp_name if \"exp_name\" in locals() else \"-\",\n",
    "        \"Task\": \"Regression\",\n",
    "        \"Model\": \"HistGradientBoostingRegressor\",\n",
    "        \"Technique\": \"SubsampledCluster\",\n",
    "        \"Precision\": None,\n",
    "        \"Recall\": None,\n",
    "        \"Score\": None,\n",
    "        \"R2\": res[\"r2\"],\n",
    "        \"MSE\": res[\"mse\"],\n",
    "    })\n",
    "\n",
    "summary_rows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
